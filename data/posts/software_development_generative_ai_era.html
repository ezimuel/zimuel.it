<post date="2025-09-24" img="background_function_call.jpg" title="Software Development in the GenAI Era" sub="Governing Probability and New Agentic Architectures">

<p>The advent of Generative AI (GenAI) is triggering a paradigm shift in software engineering. We are moving from a world dominated by deterministic logic, where every line of code executes a precise instruction, to an ecosystem where we integrate components based on probabilistic models. This transition forces us to radically rethink architectures, patterns, and best practices, especially as we put AI agents into production without a solid understanding of how they work and how to control them. The risk of exposing critical decision points to a non-deterministic system without adequate control mechanisms is one of the greatest challenges we face.</p>

<h2>Probabilistic models</h2>

<p>Unlike traditional software, GenAI models like Large Language Models (LLMs) are inherently probabilistic. They do not provide a single, certain result but instead assign a probability to every possible outcome. For LLMs, this probability is associated with generating "tokens" (words or parts of words) in sequence, based on the preceding context</p>

<p>This means that <strong>error is an intrinsic feature of the model</strong>. An LLM might generate a plausible but incorrect answer, or even "hallucinate" information. The challenge for developers is no longer just preventing logical bugs, but estimating and managing this probability of error. Some approaches to estimation include:</p>

<ul>
    <li><strong>Benchmarks</strong>: standardized question-and-answer sets, like the GPQA (Graduate-Level Google-Proof Q&A Benchmark), can be used to measure a model's accuracy on complex tasks</li> 
    <li><strong>LLM as a judge</strong>: a second LLM can be used to evaluate the quality and correctness of the answer generated by the first, often against a pre-made set of questions and answers</li>
    <li><strong>Verifying the output</strong>: if the output is a code, it can be executed to check its correctness.</li>
 </ul>

This probabilistic nature introduces new risks, such as "vibe coding". This is an iterative development practice where a developer prompts a model to generate and fix code without a deep understanding of the output. While useful for prototypes, this approach tends to generate architecturally complex ("spaghetti code") and risky code to put into production without rigorous human oversight
.
<h2>The New Architecture: Pillars of AI Agents</h2>

<p>Modern software architectures are evolving to leverage LLMs not just as text generators, but as reasoning engines for autonomous agents.</p>
<p>These "Agentic AI" systems can orchestrate complex, multi-step tasks and interact with the outside world. Their capabilities are founded on a few key (emergent) properties of large-scale language models:</p>
<ol>
    <li>
        <p><strong>Task Decomposition:</strong> The most advanced models, often called Large Reasoning Models (LRMs), are trained to break down a complex problem into simpler, more manageable sub-problems. A common architectural pattern that implements this concept is <strong>ReAct (Reason + Action)</strong>, a loop where the agent first "reasons" about the next step (Reason) and then executes an action to achieve the goal (Act).</p>
        <p>A common architectural pattern that implements this concept is <strong>ReAct (Reason + Action)</strong>, a loop where the agent first "reasons" about the next step (Reason) and then executes an action to achieve the goal (Act).</p>
        <p>More details in Shunyu Yao et al., <a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a>, 2022.</p>
    </li>
    <li><p><strong>Tool Invocation</strong>: One of the most revolutionary capabilities of LLMs is their emergent ability to "call tools"—generating structured function calls when they realize they need external data or actions to complete a task.</p>
        <p>The process works in a 3-steps dance:</p>
        <ol>
            <li>The LLM recognizes the need to use a tool based on the function's description and parameters. It suggests the function call with the correct arguments, extracted from the user's request.</li>
            <li>An external, deterministic, and controlled system executes the function (e.g. a Python function). This is a crucial architectural control point where validation logic, human feedback, or security checks can be inserted.</li>
            <li>The result of the execution is fed back into the LLM's context, allowing it to formulate an informed and accurate final response.</li>
        </ol>
        <p>More details in Enrico Zimuel, <a href="/blog/tool_calling_AI_agents">Tools calling in Agentic AI</a>, 2025.</p>
    </li>
    <li>
         <p><strong>Self-Correction based on Feedback</strong>: This concept is tightly linked to the ReAct cycle. After executing an action (Act), the agent "observes" the result.
         This output serves as feedback, which is integrated into the agent's context and informs the next reasoning step (Reason). This allows the agent to dynamically correct its action plan, learning from the outcomes of its operations to achieve the final goal more effectively.</p>
         <p>More details in Renat Aksitov et al. <a href="https://arxiv.org/pdf/2312.10003v1">ReST meets ReAct: self-improvement for multi-step reasoning LLM agents</a>, 2023.</p>
    </li>
</ol>

<h2>Redefining Best Practices: From Intuition to "AgentOps"</h2>

Putting such complex and non-deterministic systems into production requires a shift in mindset, moving beyond intuitive "vibe-testing" to embrace a rigorous engineering approach
. This has given rise to a new operational methodology: Agent Operations (AgentOps), which adapts the principles of DevOps and MLOPs to the unique challenges of AI agents
.
A robust evaluation framework, as proposed by AgentOps, must operate on multiple layers
:
• Component-level evaluation: Deterministic unit tests for tools and API integrations to ensure they are not the source of errors
.
• Trajectory evaluation: Analysis of the agent's reasoning process correctness. Did the agent choose the right tool? Did it extract the correct parameters? Was its "chain of thought" logical?
.
• Outcome evaluation: Verification of the semantic and factual correctness of the final response. Is it helpful, accurate, and based on verifiable data (grounded)?
. Here, techniques like Retrieval-Augmented Generation (RAG) are fundamental to connect the model to reliable data sources and reduce hallucinations
.
• System-level monitoring: Once in production, it is essential to continuously monitor performance, tool failure rates, latency, and user feedback to detect behavioral drift
.
<h2>Conclusion</h2>
<p>Software development in the GenAI era requires us to become architects of systems that orchestrate probabilistic reasoning engines. Success no longer depends solely on writing flawless code, but on our ability to govern uncertainty. This demands a mastery of agentic architectures built on task decomposition, tool invocation, and self-correction. Above all, it requires adopting disciplined engineering practices like AgentOps to ensure reliability, security, and control. As Shalini Kurapati, CEO of Clearbox AI, stated, "we need more developers in the AI era, not less," but their skills will need to evolve to include new professional roles like the Context Engineer, who specializes in providing the right context for models to operate effectively</p>

